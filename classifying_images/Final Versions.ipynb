{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38cdcdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from keras.applications import mobilenet\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import layers, models\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from csv import reader\n",
    "\n",
    "# libraries for outputting camera images\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pylab as pl\n",
    "\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f6c9bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=224\n",
    "n_inst = 424\n",
    "n_cats = 6\n",
    "dir_name = f\"finalModel_n{n_inst}_{n_cats}cats\"\n",
    "columns = [str(i) for i in range(n_cats)]\n",
    "\n",
    "# 'data/train_cv_224/training_labels_handOverFace_n424.csv'\n",
    "if n_inst == 871:\n",
    "    csv_filename = f'data/train_cv_224/handOverFace_n871_{n_cats}cats_1.csv'\n",
    "else:\n",
    "    csv_filename = f'data/train_cv_224/training_labels_handOverFace_n424_{n_cats}cats.csv'\n",
    "targetDir = 'data/train_cv_224'\n",
    "\n",
    "train_data = pd.read_csv(csv_filename, sep=',', header=0)\n",
    "\n",
    "# settuping the kfolds\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "idg = ImageDataGenerator(preprocessing_function=mobilenet.preprocess_input)\n",
    "idg_train = ImageDataGenerator(\n",
    "    preprocessing_function=mobilenet.preprocess_input, width_shift_range=[-25, 25], \n",
    "    height_shift_range=[-25, 25], horizontal_flip=True, brightness_range=[0.6,1.3],\n",
    "    channel_shift_range=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df025ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_model():\n",
    "    base_model = mobilenet.MobileNet(weights='imagenet', \n",
    "                                     include_top=False, \n",
    "                                     input_shape=(img_size, img_size, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dense(len(columns), activation='softmax')(x)\n",
    "    model = models.Model(inputs=base_model.input, outputs=x)\n",
    "    \n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=dir_name+'/mobileNet_n'+str(n_inst)+'_'+str(n_cats)+'cats-{epoch:02d}-{val_loss:.2f}.h5', \n",
    "    verbose=1, monitor='val_accuracy', save_best_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c32cf595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2289 validated image filenames.\n",
      "Found 255 validated image filenames.\n",
      "Epoch 1/20\n",
      "71/71 [==============================] - 34s 424ms/step - loss: 0.8283 - accuracy: 0.7108 - val_loss: 1.0786 - val_accuracy: 0.6116\n",
      "\n",
      "Epoch 00001: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-01-1.08.h5\n",
      "Epoch 2/20\n",
      "71/71 [==============================] - 28s 394ms/step - loss: 0.1856 - accuracy: 0.9322 - val_loss: 0.8021 - val_accuracy: 0.7366\n",
      "\n",
      "Epoch 00002: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-02-0.80.h5\n",
      "Epoch 3/20\n",
      "71/71 [==============================] - 28s 402ms/step - loss: 0.1431 - accuracy: 0.9480 - val_loss: 0.2660 - val_accuracy: 0.9062\n",
      "\n",
      "Epoch 00003: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-03-0.27.h5\n",
      "Epoch 4/20\n",
      "71/71 [==============================] - 30s 416ms/step - loss: 0.0979 - accuracy: 0.9629 - val_loss: 0.2893 - val_accuracy: 0.9196\n",
      "\n",
      "Epoch 00004: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-04-0.29.h5\n",
      "Epoch 5/20\n",
      "71/71 [==============================] - 31s 440ms/step - loss: 0.0608 - accuracy: 0.9790 - val_loss: 0.5677 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00005: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-05-0.57.h5\n",
      "Epoch 6/20\n",
      "71/71 [==============================] - 34s 484ms/step - loss: 0.0562 - accuracy: 0.9804 - val_loss: 0.4983 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00006: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-06-0.50.h5\n",
      "Epoch 7/20\n",
      "71/71 [==============================] - 38s 540ms/step - loss: 0.0643 - accuracy: 0.9818 - val_loss: 0.3263 - val_accuracy: 0.9062\n",
      "\n",
      "Epoch 00007: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-07-0.33.h5\n",
      "Epoch 8/20\n",
      "71/71 [==============================] - 31s 431ms/step - loss: 0.0559 - accuracy: 0.9783 - val_loss: 0.3993 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00008: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-08-0.40.h5\n",
      "Epoch 9/20\n",
      "71/71 [==============================] - 31s 431ms/step - loss: 0.0330 - accuracy: 0.9892 - val_loss: 0.2504 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00009: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-09-0.25.h5\n",
      "Epoch 10/20\n",
      "71/71 [==============================] - 30s 421ms/step - loss: 0.0577 - accuracy: 0.9839 - val_loss: 0.2397 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00010: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-10-0.24.h5\n",
      "Epoch 11/20\n",
      "71/71 [==============================] - 30s 427ms/step - loss: 0.0389 - accuracy: 0.9894 - val_loss: 0.2766 - val_accuracy: 0.9152\n",
      "\n",
      "Epoch 00011: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-11-0.28.h5\n",
      "Epoch 12/20\n",
      "71/71 [==============================] - 30s 423ms/step - loss: 0.0254 - accuracy: 0.9915 - val_loss: 0.2984 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00012: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-12-0.30.h5\n",
      "Epoch 13/20\n",
      "71/71 [==============================] - 30s 427ms/step - loss: 0.0151 - accuracy: 0.9972 - val_loss: 0.2252 - val_accuracy: 0.9688\n",
      "\n",
      "Epoch 00013: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-13-0.23.h5\n",
      "Epoch 14/20\n",
      "71/71 [==============================] - 31s 434ms/step - loss: 0.0355 - accuracy: 0.9904 - val_loss: 0.4669 - val_accuracy: 0.9152\n",
      "\n",
      "Epoch 00014: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-14-0.47.h5\n",
      "Epoch 15/20\n",
      "71/71 [==============================] - 31s 440ms/step - loss: 0.0110 - accuracy: 0.9951 - val_loss: 0.3320 - val_accuracy: 0.9420\n",
      "\n",
      "Epoch 00015: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-15-0.33.h5\n",
      "Epoch 16/20\n",
      "71/71 [==============================] - 32s 451ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 0.3411 - val_accuracy: 0.9330\n",
      "\n",
      "Epoch 00016: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-16-0.34.h5\n",
      "Epoch 17/20\n",
      "71/71 [==============================] - 33s 462ms/step - loss: 0.0270 - accuracy: 0.9906 - val_loss: 0.3550 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00017: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-17-0.36.h5\n",
      "Epoch 18/20\n",
      "71/71 [==============================] - 32s 452ms/step - loss: 0.0463 - accuracy: 0.9830 - val_loss: 0.4363 - val_accuracy: 0.9152\n",
      "\n",
      "Epoch 00018: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-18-0.44.h5\n",
      "Epoch 19/20\n",
      "71/71 [==============================] - 33s 460ms/step - loss: 0.0278 - accuracy: 0.9903 - val_loss: 0.5323 - val_accuracy: 0.9018\n",
      "\n",
      "Epoch 00019: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-19-0.53.h5\n",
      "Epoch 20/20\n",
      "71/71 [==============================] - 32s 450ms/step - loss: 0.0576 - accuracy: 0.9824 - val_loss: 0.4626 - val_accuracy: 0.8973\n",
      "\n",
      "Epoch 00020: saving model to finalModel_n424_6cats/mobileNet_n424_6cats-20-0.46.h5\n",
      "8/8 [==============================] - 3s 369ms/step - loss: 0.4653 - accuracy: 0.9020\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"97c6a66e-6202-4ac5-abf4-8757cbb05c4f\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"97c6a66e-6202-4ac5-abf4-8757cbb05c4f\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "### FOR GETTING A GOOD MODEL\n",
    "### FOR CROSS VALIDATION\n",
    "## running only one of it\n",
    "valid_accuracy = []\n",
    "valid_loss = []\n",
    "batch_size=32\n",
    "num_epochs=20\n",
    "num_samples = train_data.shape[0]\n",
    "\n",
    "for train_index, val_index in list(kf.split(np.zeros(num_samples), train_data)):\n",
    "    training_data = train_data.iloc[train_index]\n",
    "    validation_data = train_data.iloc[val_index]\n",
    "    \n",
    "    # one hot encoding because it is softmax    \n",
    "    train_data_generator = idg.flow_from_dataframe(dataframe=training_data, directory=targetDir,\n",
    "                                                  x_col=\"filename\", \n",
    "                                                   y_col=columns,\n",
    "                                                  shuffle=True,\n",
    "                                                  target_size=(img_size, img_size), \n",
    "                                                   batch_size=batch_size, \n",
    "                                                  class_mode='raw')\n",
    "    valid_data_generator = idg.flow_from_dataframe(dataframe=validation_data, directory=targetDir,\n",
    "                                                  x_col=\"filename\", \n",
    "                                                   y_col=columns,\n",
    "                                                  shuffle=True,\n",
    "                                                  target_size=(img_size, img_size), \n",
    "                                                   batch_size=batch_size,\n",
    "                                                  class_mode='raw')\n",
    "    \n",
    "    model = create_new_model()\n",
    "    \n",
    "    step_size = train_data_generator.n//train_data_generator.batch_size\n",
    "    step_size_valid = valid_data_generator.n//valid_data_generator.batch_size\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_data_generator, epochs=num_epochs, steps_per_epoch=step_size, \n",
    "                        validation_data=valid_data_generator,\n",
    "                       validation_steps=step_size_valid,\n",
    "                        callbacks=[model_checkpoint_callback])\n",
    "    \n",
    "    results = model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(model.metrics_names, results))\n",
    "    \n",
    "    valid_accuracy.append(results['accuracy'])\n",
    "    valid_loss.append(results['loss'])\n",
    "    \n",
    "    output_dir_name = os.path.join(dir_name, f\"history.pickle\")\n",
    "    with open(output_dir_name, \"wb\") as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34b05c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_name = os.path.join(dir_name, f\"history.pickle\")\n",
    "with open(output_dir_name, \"wb\") as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
